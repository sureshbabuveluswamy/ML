{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "748b1f6d-0d24-40f4-b1ea-b55fa327845d",
   "metadata": {
    "panel-layout": {
     "height": 51.140625,
     "visible": true,
     "width": 100
    }
   },
   "source": [
    "Linear Regression Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8008ad38-e547-43bc-ac0b-4916e549340a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a6e97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import model related libraries\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from math import pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "574d7a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RMSLE function definition\n",
    "#RMSLE = sqrt( (1/n) * Σ (log(pred+1) - log(actual+1))² ) \n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    y_pred = np.where(y_pred<0, 0, y_pred)\n",
    "    return np.sqrt(np.mean((np.log1p(y_pred) - np.log1p(y_true))**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f404242",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'bike_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# importing data to dataframe using pandas  \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbike_train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbike_test.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bike_train.csv'"
     ]
    }
   ],
   "source": [
    "# importing data to dataframe using pandas  \n",
    "train = pd.read_csv(\"bike_train.csv\")\n",
    "test = pd.read_csv(\"bike_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f97418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the dataset\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b25ac99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the training dataset\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6849d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train.info()) #2.4 Summary of  Training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad3707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (train.describe()) #2.5 Statistical Summary of all numerical attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f57a80",
   "metadata": {},
   "source": [
    "Q1. Examine dataset size, missing values, and feature types. \n",
    "answer : no missing value ,  categorical feature are season, holiday, workingday, weather & continuous variable are temp, atemp, humidity, windspeed\n",
    "Q2. Visualize relationships between key features and the target variable (count).\n",
    "\n",
    "Q3: Suggest which variables are likely to be most informative.\n",
    "Holidays and working day plays important in categorical data\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3517b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualization of data is kept standlone for better readability\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('bike_train.csv')\n",
    "\n",
    "# Convert datetime column to datetime type and extract hour and month for temporal analysis\n",
    "train_df['datetime'] = pd.to_datetime(train_df['datetime'], format='%d/%m/%y %H:%M')\n",
    "train_df['hour'] = train_df['datetime'].dt.hour\n",
    "train_df['month'] = train_df['datetime'].dt.month\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# 1. Histograms for continuous features and target\n",
    "continuous_features = ['temp', 'atemp', 'humidity', 'windspeed', 'count']\n",
    "train_df[continuous_features].hist(bins=30, figsize=(12, 8))\n",
    "plt.suptitle('Distribution of Continuous Features and Target')\n",
    "plt.show()\n",
    "\n",
    "# 2. Boxplots of count by categorical features\n",
    "categorical_features = ['season', 'holiday', 'workingday', 'weather']\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i, feature in enumerate(categorical_features, 1):\n",
    "    plt.subplot(2, 2, i)\n",
    "    sns.boxplot(x=feature, y='count', data=train_df)\n",
    "    plt.title(f'Count by {feature.capitalize()}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Scatter plots for continuous features vs count\n",
    "plt.figure(figsize=(15, 5))\n",
    "for i, feature in enumerate(['temp', 'humidity', 'windspeed'], 1):\n",
    "    plt.subplot(1, 3, i)\n",
    "    sns.scatterplot(x=feature, y='count', data=train_df, alpha=0.3)\n",
    "    plt.title(f'Count vs {feature.capitalize()}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Line plot for average count by hour of the day\n",
    "plt.figure(figsize=(8, 5))\n",
    "hourly_counts = train_df.groupby('hour')['count'].mean()\n",
    "sns.lineplot(x=hourly_counts.index, y=hourly_counts.values)\n",
    "plt.title('Average Bike Count by Hour of the Day')\n",
    "plt.xlabel('Hour')\n",
    "plt.ylabel('Average Count')\n",
    "plt.show()\n",
    "\n",
    "# 5. Correlation heatmap for continuous variables\n",
    "plt.figure(figsize=(8, 6))\n",
    "corr = train_df[continuous_features].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Heatmap of Continuous Features')\n",
    "plt.show()\n",
    "\n",
    "# 6. Pairplot of key features and count (sample for speed)\n",
    "sns.pairplot(train_df.sample(500), vars=['temp', 'humidity', 'windspeed', 'count'])\n",
    "plt.suptitle('Pairplot of Selected Features and Count', y=1.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6a65ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your dataset uses DD/MM/YY HH:MM format, so dayfirst=True is appropriate\n",
    "train['datetime_parsed'] = pd.to_datetime(\n",
    "    train['datetime'], \n",
    "    dayfirst=True, \n",
    "    errors='coerce'\n",
    ")\n",
    "\n",
    "test['datetime_parsed'] = pd.to_datetime(\n",
    "    test['datetime'], \n",
    "    dayfirst=True, \n",
    "    errors='coerce'\n",
    ")\n",
    "#  Extract datetime features\n",
    "def add_datetime_features(df):\n",
    "    # Basic time units\n",
    "    df['hour'] = df['datetime_parsed'].dt.hour\n",
    "    df['day'] = df['datetime_parsed'].dt.day\n",
    "    df['month'] = df['datetime_parsed'].dt.month\n",
    "    df['year'] = df['datetime_parsed'].dt.year\n",
    "    \n",
    "    # Monday = 0 ... Sunday = 6\n",
    "    df['weekday'] = df['datetime_parsed'].dt.weekday\n",
    "    \n",
    "    # Weekend flag\n",
    "    df['is_weekend'] = df['weekday'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Apply for both train and test\n",
    "train = add_datetime_features(train)\n",
    "test = add_datetime_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805cc2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_interaction_features(df):\n",
    "    df[\"temp_feels_diff\"] = df[\"temp\"] - df[\"atemp\"]\n",
    "    df[\"bad_weather_peak\"] = df[\"weather\"] * df[\"hour\"]\n",
    "    df[\"humid_temp\"] = df[\"humidity\"] * df[\"temp\"]\n",
    "    df[\"work_hour\"] = df[\"workingday\"] * df[\"hour\"]\n",
    "    df[\"is_weekend\"] = (df[\"weekday\"] >= 5).astype(int)\n",
    "    df[\"weekend_hour\"] = df[\"is_weekend\"] * df[\"hour\"]\n",
    "    df[\"wind_weather\"] = df[\"windspeed\"] * df[\"weather\"]\n",
    "    df[\"temp_hour\"] = df[\"temp\"] * df[\"hour\"]\n",
    "    df[\"peak_morning\"] = df[\"hour\"].isin([7,8,9]).astype(int)\n",
    "    df[\"peak_evening\"] = df[\"hour\"].isin([17,18,19]).astype(int)\n",
    "    return df\n",
    "\n",
    "# Apply for both train and test\n",
    "train = add_datetime_features(train)\n",
    "test = add_datetime_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32904af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  causal and registered columns are not present in test set, so drop them from train set , also target is to get predicated count , \n",
    "# hence dropping these columns will avoid data leakage  \n",
    "for c in ['casual','registered']:\n",
    "    if c in train.columns:\n",
    "        train = train.drop(columns=[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d43c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Target & base features\n",
    "y = train['count']\n",
    "X = train.drop(columns=['count','datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ccc93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cyclical encoding of time features\n",
    "def cyclical_encode(series, period, prefix):\n",
    "    radians = 2 * np.pi * series / period\n",
    "    return pd.DataFrame({\n",
    "        f'{prefix}_sin': np.sin(radians),\n",
    "        f'{prefix}_cos': np.cos(radians)\n",
    "    })\n",
    "\n",
    "# Apply cyclical enc\n",
    "X_cyc = pd.concat([\n",
    "    cyclical_encode(X['hour'], 24, 'hour'),\n",
    "    cyclical_encode(X['month'], 12, 'month'),\n",
    "    cyclical_encode(X['weekday'], 7, 'weekday')], axis=1)\n",
    "\n",
    "X_model_fe = X.drop(columns=['hour','day','month','weekday']).reset_index(drop=True)\n",
    "X_model_fe = pd.concat([X_model_fe, X_cyc.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# same for test\n",
    "test_model = test.drop(columns=['datetime']).copy()\n",
    "test_cyc = pd.concat([\n",
    "    cyclical_encode(test_model['hour'], 24, 'hour'),\n",
    "    cyclical_encode(test_model['month'], 12, 'month'),\n",
    "    cyclical_encode(test_model['weekday'], 7, 'weekday')], axis=1)\n",
    "test_model = test_model.drop(columns=['hour','day','month','weekday']).reset_index(drop=True)\n",
    "test_model = pd.concat([test_model, test_cyc.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61b4c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model feature columns\n",
    "numeric_cols = ['temp','atemp','humidity','windspeed','year','day']\n",
    "cyc_cols = ['hour_sin','hour_cos','month_sin','month_cos','weekday_sin','weekday_cos']\n",
    "cat_cols = ['season','weather','holiday','workingday','is_weekend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f518a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure expected columns exist\n",
    "for col in numeric_cols + cyc_cols + cat_cols:\n",
    "    if col not in X_model_fe.columns:\n",
    "        if col in cyc_cols: X_model_fe[col] = 0.0\n",
    "        elif col in numeric_cols: X_model_fe[col] = X_model_fe[col] if col in X_model_fe.columns else 0.0\n",
    "        else: X_model_fe[col] = 0\n",
    "    if col not in test_model.columns:\n",
    "        test_model[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing pipelines for both numeric and categorical data\n",
    "numeric_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "categorical_transformer = Pipeline([('imputer', SimpleImputer(strategy='most_frequent')), ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numeric_transformer, numeric_cols + cyc_cols),\n",
    "    ('cat', categorical_transformer, cat_cols)\n",
    "], remainder='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b199c7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and validation sets\n",
    "#randoem_state=42 for reproducibility\n",
    "#test_size=0.2 means 20% data will be used for validation\n",
    "#Q5. Split data into training and validation sets and build a simple Linear Regression model.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_model_fe, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f485fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#liner regression , ridge regression , random forest and gradient boosting models\n",
    "#Q6. To improve model performance, you may try to different models and tune hyperparameters.:\n",
    "\n",
    "models = {\n",
    "    'Linear': Pipeline([('pre', preprocessor), ('model', LinearRegression())]),\n",
    "    'Ridge': Pipeline([('pre', preprocessor), ('model', RidgeCV(alphas=[0.1,1.0,10.0]))]),\n",
    "    'Polynomial Regression (Degree 2)': Pipeline([\n",
    "        ('pre', preprocessor),\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "            ('model', LinearRegression())\n",
    "    ]),\n",
    "    'Lasso Polynomial Regression': Pipeline([\n",
    "        ('pre', preprocessor),\n",
    "        ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "  \n",
    "        ('model', Lasso(alpha=0.001))\n",
    "    ]),\n",
    "    'RandomForest': Pipeline([('pre', preprocessor), ('model', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))]),\n",
    "    'GradientBoosting': Pipeline([('pre', preprocessor), ('model', GradientBoostingRegressor(n_estimators=200, random_state=42))])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf475d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7. Summarize all results (of different models tried out) in one table (RMSLE, key observations).\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_train, y_train)\n",
    "    preds = pipe.predict(X_val)\n",
    "    print(name, \"RMSLE:\", rmsle(y_val, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbed9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on \n",
    "final_pipe = Pipeline([('pre', preprocessor), ('model', RandomForestRegressor(n_estimators=200, random_state=42, n_jobs=-1))])\n",
    "final_pipe.fit(X_model_fe, y)\n",
    "test_preds = final_pipe.predict(test_model[numeric_cols + cyc_cols + cat_cols])\n",
    "test_preds = np.where(test_preds < 0, 0, test_preds)\n",
    "test_preds_rounded = np.round(test_preds).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output submission file\n",
    "submission_out = \"submission.csv\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1802e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'datetime': test['datetime'], 'count_predicted': test_preds_rounded})\n",
    "submission.to_csv(submission_out, index=False)\n",
    "print(\"Saved submission to:\", submission_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d3270d",
   "metadata": {},
   "source": [
    "Q7. Summarize all results\n",
    "Modelname\t                          RMSLE\t                        Observations\n",
    "Liner Regression \t                     1.12\t    Baseline linear, cannot capture nonlinear or time effects\n",
    "Ridge\t                                 1.11\t    small improvement, regularization reduces overfit slightly\n",
    "Polynomial Regression (Degree 2)\t     1.23\t    Unstable, tends to overfit, sensitive to feature scaling\n",
    "Lasso Polynomial Regression \t         1.23\t    Better than plain polynomial, regularization helps but still unstable\n",
    "RandomForest \t                         0.39\t    Best model; captures nonlinearity, interactions, and categorical splits\n",
    "Gradient Boosting\t                     0.83\t    Reasonably good for nonlinearity, less robust to noise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q8. Plot residuals for the best model.\n",
    "residuals = y_val- preds\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(residuals, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.title('Residuals Histogram - Best Model')\n",
    "plt.xlabel('Residuals (Actual - Predicted)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab59f6ee",
   "metadata": {},
   "source": [
    "Q9 .Explain why the winning model performs better.\n",
    "\n",
    "The winning model, Random Forest, performs better because it can capture complex nonlinear relationships and interactions among features that simpler models like linear regression cannot. Unlike linear models, which assume additive and independent effects for each feature, Random Forest aggregates multiple decision trees that split the data based on feature values, allowing it to model arbitrary interactions between numeric and categorical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b3f4c5",
   "metadata": {},
   "source": [
    "Q10: Why does RMSLE penalize under-predictions more gently than RMSE?\n",
    "\n",
    "RMSLE uses logarithmic errors, so it reduces the penalty when a predicted value is less than the true value (especially for large true values), making it less sensitive to large under-predictions compared to RMSE, which squares all errors equally.\n",
    "\n",
    "Q11: What are the trade-offs between model simplicity and predictive power?\n",
    "\n",
    "Simpler models (like linear regression) are easier to interpret and less likely to overfit, but may miss nonlinear patterns and feature interactions. More complex models (e.g., Random Forest) potentially capture more data structure at the cost of explainability and risk of overfitting if not well-tuned.\n",
    "\n",
    "\n",
    "Q12: Why can’t Linear Regression alone capture time-of-day effects effectively?\n",
    "\n",
    "Linear regression cannot represent cyclic or periodic patterns (e.g., rush hours, night use) because it assumes additive and linear feature effects. Time-of-day effects are typically nonlinear and cyclical, requiring either feature engineering (cyclical encoding) or more flexible models to represent accurately.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "panel-cell-order": [
   "748b1f6d-0d24-40f4-b1ea-b55fa327845d"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
