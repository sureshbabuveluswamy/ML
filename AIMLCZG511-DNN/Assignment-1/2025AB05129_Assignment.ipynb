{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deep Neural Networks - Programming Assignment\n",
        "## Comparing Linear Models and Multi-Layer Perceptrons\n",
        "\n",
        "**Student Name:** ___________________  \n",
        "**Student ID:** ___________________  \n",
        "**Date:** ___________________\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è IMPORTANT INSTRUCTIONS\n",
        "\n",
        "1. **Complete ALL sections** marked with `TODO`\n",
        "2. **DO NOT modify** the `get_assignment_results()` function structure\n",
        "3. **Fill in all values accurately** - these will be auto-verified\n",
        "4. **After submission**, you'll receive a verification quiz based on YOUR results\n",
        "5. **Run all cells** before submitting (Kernel ‚Üí Restart & Run All)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Libraries imported successfully\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "print('‚úì Libraries imported successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Dataset Selection and Loading\n",
        "\n",
        "**Requirements:**\n",
        "- ‚â•500 samples\n",
        "- ‚â•5 features\n",
        "- Public dataset (UCI/Kaggle)\n",
        "- Regression OR Classification problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset: House Prices\n",
            "Source: Kaggle\n",
            "Samples: 545, Features: 12\n",
            "Problem Type: Regression\n",
            "Primary Metric: mse,rmse,mae\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>price</th>\n",
              "      <th>area</th>\n",
              "      <th>bedrooms</th>\n",
              "      <th>bathrooms</th>\n",
              "      <th>stories</th>\n",
              "      <th>mainroad</th>\n",
              "      <th>guestroom</th>\n",
              "      <th>basement</th>\n",
              "      <th>hotwaterheating</th>\n",
              "      <th>airconditioning</th>\n",
              "      <th>parking</th>\n",
              "      <th>prefarea</th>\n",
              "      <th>furnishingstatus</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>13300000</td>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12250000</td>\n",
              "      <td>8960</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12250000</td>\n",
              "      <td>9960</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>semi-furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12215000</td>\n",
              "      <td>7500</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>3</td>\n",
              "      <td>yes</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11410000</td>\n",
              "      <td>7420</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>2</td>\n",
              "      <td>no</td>\n",
              "      <td>furnished</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
              "0  13300000  7420         4          2        3      yes        no       no   \n",
              "1  12250000  8960         4          4        4      yes        no       no   \n",
              "2  12250000  9960         3          2        2      yes        no      yes   \n",
              "3  12215000  7500         4          2        2      yes        no      yes   \n",
              "4  11410000  7420         4          1        2      yes       yes      yes   \n",
              "\n",
              "  hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
              "0              no             yes        2      yes        furnished  \n",
              "1              no             yes        3       no        furnished  \n",
              "2              no              no        2      yes   semi-furnished  \n",
              "3              no             yes        3      yes        furnished  \n",
              "4              no             yes        2       no        furnished  "
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TODO: Load your dataset\n",
        "# Example: data = pd.read_csv('your_dataset.csv')\n",
        "data = pd.read_csv('Housing.csv')\n",
        "\n",
        "# Dataset information (TODO: Fill these)\n",
        "dataset_name = \"House Prices\"  # e.g., \"House Price prediction\"\n",
        "dataset_source = \"Kaggle\"  # e.g., \"UCI ML Repository\"\n",
        "n_samples = 545     # Total number of rows\n",
        "n_features = 12     # Number of features (excluding target)\n",
        "problem_type = \"Regression\"  # \"regression\" or \"binary_classification\" or \"multiclass_classification\"\n",
        "\n",
        "# Problem statement (TODO: Write 2-3 sentences)\n",
        "problem_statement = \"To predict the price of a house based on various features like area, bedrooms, bathrooms, etc.\"\n",
        "\n",
        "\n",
        "# Primary evaluation metric (TODO: Fill this)\n",
        "primary_metric = \"mse,rmse,mae\"  # e.g., \"recall\", \"accuracy\", \"rmse\", \"r2\"\n",
        "\n",
        "# Metric justification (TODO: Write 2-3 sentences)\n",
        "metric_justification = \"MAE -Choosen since its robust to outliers, RMSE & MSE Square errors for senstivity to large errors\"\n",
        "\n",
        "\n",
        "print(f\"Dataset: {dataset_name}\")\n",
        "print(f\"Source: {dataset_source}\")\n",
        "print(f\"Samples: {n_samples}, Features: {n_features}\")\n",
        "print(f\"Problem Type: {problem_type}\")\n",
        "print(f\"Primary Metric: {primary_metric}\")\n",
        "# Display the first 5 rows of the training dataset\n",
        "data.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Data Preprocessing\n",
        "\n",
        "Preprocess your data:\n",
        "1. Handle missing values\n",
        "2. Encode categorical variables\n",
        "3. Split into train/test sets\n",
        "4. Scale features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train samples: 436\n",
            "Test samples: 109\n",
            "Split ratio: 80.0%\n"
          ]
        }
      ],
      "source": [
        "# TODO: Preprocess your data\n",
        "#1. Handle missing values /Drop Missing values. \n",
        "data = data.dropna()\n",
        "\n",
        "# 2. Separate features and target\n",
        "X = data.drop('price', axis=1)\n",
        "y = data['price'].values \n",
        "\n",
        "# 3.Encode categorical variables, for better classification\n",
        "categorical_cols = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', \n",
        "                    'airconditioning', 'prefarea']\n",
        "for col in categorical_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col])\n",
        "\n",
        "# 4.One-hot encode furnishingstatus\n",
        "X = pd.get_dummies(X, columns=['furnishingstatus'], drop_first=True)\n",
        "X = X.values\n",
        "\n",
        "\n",
        "# TODO: Train-test split(80-20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# TODO: Feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Fill these after preprocessing\n",
        "train_samples = X_train.shape[0]      # Number of training samples\n",
        "test_samples = X_test.shape[0]       # Number of test samples\n",
        "train_test_ratio = .8\n",
        "\n",
        "print(f\"Train samples: {train_samples}\")\n",
        "print(f\"Test samples: {test_samples}\")\n",
        "print(f\"Split ratio: {train_test_ratio:.1%}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Baseline Model Implementation\n",
        "\n",
        "Implement from scratch (NO sklearn models!):\n",
        "- Linear Regression (for regression)\n",
        "- Logistic Regression (for binary classification)\n",
        "- Softmax Regression (for multiclass classification)\n",
        "\n",
        "**Must include:**\n",
        "- Forward pass (prediction)\n",
        "- Loss computation\n",
        "- Gradient computation\n",
        "- Gradient descent loop\n",
        "- Loss tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Baseline model class defined\n"
          ]
        }
      ],
      "source": [
        "class BaselineModel:\n",
        "    \"\"\"\n",
        "    Baseline linear model with gradient descent\n",
        "    Implement: Linear/Logistic/Softmax Regression\n",
        "    \"\"\"\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.lr = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "        self.loss_history = []\n",
        "    \n",
        "    def fit(self, X, y,verbose=True):\n",
        "        \"\"\"\n",
        "        TODO: Implement gradient descent training\n",
        "        \n",
        "        Steps:\n",
        "        1. Initialize weights and bias\n",
        "        2. For each iteration:\n",
        "           a. Compute predictions (forward pass)\n",
        "           b. Compute loss\n",
        "           c. Compute gradients\n",
        "           d. Update weights and bias\n",
        "           e. Store loss in self.loss_history\n",
        "        \n",
        "        Must populate self.loss_history with loss at each iteration!\n",
        "        \"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        \n",
        "        # TODO: Initialize parameters\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "        \n",
        "        # TODO: Implement gradient descent loop\n",
        "           # Gradient descent loop\n",
        "        for iteration in range(self.n_iterations):\n",
        "            # Forward pass: predictions\n",
        "            y_pred = X @ self.weights + self.bias\n",
        "\n",
        "            # Compute loss (MSE)\n",
        "            mse_loss = np.mean((y_pred - y) ** 2)\n",
        "            self.loss_history.append(mse_loss)\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (2 / n_samples) * X.T @ (y_pred - y)\n",
        "            db = (2 / n_samples) * np.sum(y_pred - y)\n",
        "\n",
        "            # Update weights: w = w - lr * grad_w\n",
        "            self.weights = self.weights - self.lr * dw\n",
        "            self.bias = self.bias - self.lr * db\n",
        "      \n",
        "             # Print progress\n",
        "            if verbose and (iteration % 100 == 0 or iteration == self.n_iterations - 1):\n",
        "                print(f\"Iteration {iteration:4d} | Loss: {mse_loss:.4f}\")\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\nTraining completed!\")\n",
        "            print(f\"Final Loss: {self.loss_history[-1]:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        return X @ self.weights + self.bias\n",
        "        \"\"\"\n",
        "        TODO: Implement prediction\n",
        "        \n",
        "        For regression: return linear_output\n",
        "        For classification: return class probabilities or labels\n",
        "        \"\"\"\n",
        "        pass  # Replace with your implementation\n",
        "\n",
        "print(\"‚úì Baseline model class defined\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training baseline model...\n",
            "Iteration    0 | Loss: 25234792406487.6133\n",
            "Iteration  100 | Loss: 1370436805999.2383\n",
            "Iteration  200 | Loss: 976712970341.8345\n",
            "Iteration  300 | Loss: 968699659414.5281\n",
            "Iteration  400 | Loss: 968397479641.6851\n",
            "Iteration  500 | Loss: 968365022913.4082\n",
            "Iteration  600 | Loss: 968359489336.0347\n",
            "Iteration  700 | Loss: 968358442967.7993\n",
            "Iteration  800 | Loss: 968358238867.8748\n",
            "Iteration  900 | Loss: 968358198503.9960\n",
            "Iteration 1000 | Loss: 968358190459.5011\n",
            "Iteration 1100 | Loss: 968358188847.5945\n",
            "Iteration 1200 | Loss: 968358188523.1134\n",
            "Iteration 1300 | Loss: 968358188457.4926\n",
            "Iteration 1400 | Loss: 968358188444.1562\n",
            "Iteration 1500 | Loss: 968358188441.4310\n",
            "Iteration 1600 | Loss: 968358188440.8707\n",
            "Iteration 1700 | Loss: 968358188440.7549\n",
            "Iteration 1800 | Loss: 968358188440.7308\n",
            "Iteration 1900 | Loss: 968358188440.7256\n",
            "Iteration 2000 | Loss: 968358188440.7246\n",
            "Iteration 2100 | Loss: 968358188440.7242\n",
            "Iteration 2200 | Loss: 968358188440.7242\n",
            "Iteration 2300 | Loss: 968358188440.7242\n",
            "Iteration 2400 | Loss: 968358188440.7242\n",
            "Iteration 2500 | Loss: 968358188440.7242\n",
            "Iteration 2600 | Loss: 968358188440.7244\n",
            "Iteration 2700 | Loss: 968358188440.7242\n",
            "Iteration 2800 | Loss: 968358188440.7244\n",
            "Iteration 2900 | Loss: 968358188440.7242\n",
            "Iteration 3000 | Loss: 968358188440.7242\n",
            "Iteration 3100 | Loss: 968358188440.7242\n",
            "Iteration 3200 | Loss: 968358188440.7242\n",
            "Iteration 3300 | Loss: 968358188440.7242\n",
            "Iteration 3400 | Loss: 968358188440.7244\n",
            "Iteration 3500 | Loss: 968358188440.7242\n",
            "Iteration 3600 | Loss: 968358188440.7242\n",
            "Iteration 3700 | Loss: 968358188440.7244\n",
            "Iteration 3800 | Loss: 968358188440.7242\n",
            "Iteration 3900 | Loss: 968358188440.7242\n",
            "Iteration 4000 | Loss: 968358188440.7242\n",
            "Iteration 4100 | Loss: 968358188440.7242\n",
            "Iteration 4200 | Loss: 968358188440.7242\n",
            "Iteration 4300 | Loss: 968358188440.7242\n",
            "Iteration 4400 | Loss: 968358188440.7242\n",
            "Iteration 4500 | Loss: 968358188440.7242\n",
            "Iteration 4600 | Loss: 968358188440.7242\n",
            "Iteration 4700 | Loss: 968358188440.7242\n",
            "Iteration 4800 | Loss: 968358188440.7242\n",
            "Iteration 4900 | Loss: 968358188440.7242\n",
            "Iteration 5000 | Loss: 968358188440.7242\n",
            "Iteration 5100 | Loss: 968358188440.7242\n",
            "Iteration 5200 | Loss: 968358188440.7242\n",
            "Iteration 5300 | Loss: 968358188440.7242\n",
            "Iteration 5400 | Loss: 968358188440.7242\n",
            "Iteration 5500 | Loss: 968358188440.7242\n",
            "Iteration 5600 | Loss: 968358188440.7242\n",
            "Iteration 5700 | Loss: 968358188440.7242\n",
            "Iteration 5800 | Loss: 968358188440.7242\n",
            "Iteration 5900 | Loss: 968358188440.7242\n",
            "Iteration 6000 | Loss: 968358188440.7242\n",
            "Iteration 6100 | Loss: 968358188440.7242\n",
            "Iteration 6200 | Loss: 968358188440.7242\n",
            "Iteration 6300 | Loss: 968358188440.7242\n",
            "Iteration 6400 | Loss: 968358188440.7242\n",
            "Iteration 6500 | Loss: 968358188440.7242\n",
            "Iteration 6600 | Loss: 968358188440.7242\n",
            "Iteration 6700 | Loss: 968358188440.7242\n",
            "Iteration 6800 | Loss: 968358188440.7242\n",
            "Iteration 6900 | Loss: 968358188440.7242\n",
            "Iteration 7000 | Loss: 968358188440.7242\n",
            "Iteration 7100 | Loss: 968358188440.7242\n",
            "Iteration 7200 | Loss: 968358188440.7242\n",
            "Iteration 7300 | Loss: 968358188440.7242\n",
            "Iteration 7400 | Loss: 968358188440.7242\n",
            "Iteration 7500 | Loss: 968358188440.7242\n",
            "Iteration 7600 | Loss: 968358188440.7242\n",
            "Iteration 7700 | Loss: 968358188440.7242\n",
            "Iteration 7800 | Loss: 968358188440.7242\n",
            "Iteration 7900 | Loss: 968358188440.7242\n",
            "Iteration 8000 | Loss: 968358188440.7242\n",
            "Iteration 8100 | Loss: 968358188440.7242\n",
            "Iteration 8200 | Loss: 968358188440.7242\n",
            "Iteration 8300 | Loss: 968358188440.7242\n",
            "Iteration 8400 | Loss: 968358188440.7242\n",
            "Iteration 8500 | Loss: 968358188440.7242\n",
            "Iteration 8600 | Loss: 968358188440.7242\n",
            "Iteration 8700 | Loss: 968358188440.7242\n",
            "Iteration 8800 | Loss: 968358188440.7242\n",
            "Iteration 8900 | Loss: 968358188440.7242\n",
            "Iteration 9000 | Loss: 968358188440.7242\n",
            "Iteration 9100 | Loss: 968358188440.7242\n",
            "Iteration 9200 | Loss: 968358188440.7242\n",
            "Iteration 9300 | Loss: 968358188440.7242\n",
            "Iteration 9400 | Loss: 968358188440.7242\n",
            "Iteration 9500 | Loss: 968358188440.7242\n",
            "Iteration 9600 | Loss: 968358188440.7242\n",
            "Iteration 9700 | Loss: 968358188440.7242\n",
            "Iteration 9800 | Loss: 968358188440.7242\n",
            "Iteration 9900 | Loss: 968358188440.7242\n",
            "Iteration 9999 | Loss: 968358188440.7242\n",
            "\n",
            "Training completed!\n",
            "Final Loss: 968358188440.7242\n",
            "‚úì Baseline training completed in 0.29s\n",
            "‚úì Loss decreased from 25234792406487.6133 to 968358188440.7242\n"
          ]
        }
      ],
      "source": [
        "# Train baseline model\n",
        "print(\"Training baseline model...\")\n",
        "baseline_start_time = time.time()\n",
        "\n",
        "# TODO: Initialize and train your baseline model\n",
        "# Loss not reducing after 250 iteration , hence kept iteration as 1000\n",
        "baseline_model = BaselineModel(learning_rate=0.01, n_iterations=10000)\n",
        "baseline_model.fit(X_train, y_train)\n",
        "\n",
        "# TODO: Make predictions\n",
        "baseline_predictions = baseline_model.predict(X_test)\n",
        "\n",
        "baseline_training_time = time.time() - baseline_start_time\n",
        "print(f\"‚úì Baseline training completed in {baseline_training_time:.2f}s\")\n",
        "print(f\"‚úì Loss decreased from {baseline_model.loss_history[0]:.4f} to {baseline_model.loss_history[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Multi-Layer Perceptron Implementation\n",
        "\n",
        "Implement MLP from scratch with:\n",
        "- At least 1 hidden layer\n",
        "- ReLU activation for hidden layers\n",
        "- Appropriate output activation\n",
        "- Forward propagation\n",
        "- Backward propagation\n",
        "- Gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    \"\"\"\n",
        "    Multi-Layer Perceptron implemented from scratch\n",
        "    \"\"\"\n",
        "    def __init__(self, architecture, learning_rate=0.01, n_iterations=1000):\n",
        "        \"\"\"\n",
        "        architecture: list [input_size, hidden1, hidden2, ..., output_size]\n",
        "        Example: [30, 16, 8, 1] means:\n",
        "            - 30 input features\n",
        "            - Hidden layer 1: 16 neurons\n",
        "            - Hidden layer 2: 8 neurons\n",
        "            - Output layer: 1 neuron\n",
        "        \"\"\"\n",
        "        self.architecture = architecture\n",
        "        self.lr = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.parameters = {}\n",
        "        self.loss_history = []\n",
        "        self.cache = {}\n",
        "    \n",
        "    def initialize_parameters(self):\n",
        "        \"\"\"\n",
        "        TODO: Initialize weights and biases for all layers\n",
        "        \n",
        "        For each layer l:\n",
        "        - W[l]: weight matrix of shape (n[l], n[l-1])\n",
        "        - b[l]: bias vector of shape (n[l], 1)\n",
        "        \n",
        "        Store in self.parameters dictionary\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        for l in range(1, len(self.architecture)):\n",
        "            # TODO: Initialize weights and biases\n",
        "            # self.parameters[f'W{l}'] = ...\n",
        "            # self.parameters[f'b{l}'] = ...\n",
        "            pass\n",
        "    \n",
        "    def relu(self, Z):\n",
        "        \"\"\"ReLU activation function\"\"\"\n",
        "        return np.maximum(0, Z)\n",
        "    \n",
        "    def relu_derivative(self, Z):\n",
        "        \"\"\"ReLU derivative\"\"\"\n",
        "        return (Z > 0).astype(float)\n",
        "    \n",
        "    def sigmoid(self, Z):\n",
        "        \"\"\"Sigmoid activation (for binary classification output)\"\"\"\n",
        "        return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
        "    \n",
        "    def forward_propagation(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Implement forward pass through all layers\n",
        "        \n",
        "        For each layer:\n",
        "        1. Z[l] = W[l] @ A[l-1] + b[l]\n",
        "        2. A[l] = activation(Z[l])\n",
        "        \n",
        "        Store Z and A in self.cache for backpropagation\n",
        "        Return final activation A[L]\n",
        "        \"\"\"\n",
        "        self.cache['A0'] = X\n",
        "        \n",
        "        # TODO: Implement forward pass\n",
        "        # for l in range(1, len(self.architecture)):\n",
        "        #     ...\n",
        "        \n",
        "        pass  # Replace with your implementation\n",
        "    \n",
        "    def backward_propagation(self, X, y):\n",
        "        \"\"\"\n",
        "        TODO: Implement backward pass to compute gradients\n",
        "        \n",
        "        Starting from output layer, compute:\n",
        "        1. dZ[l] for each layer\n",
        "        2. dW[l] = dZ[l] @ A[l-1].T / m\n",
        "        3. db[l] = sum(dZ[l]) / m\n",
        "        \n",
        "        Return dictionary of gradients\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        grads = {}\n",
        "        \n",
        "        # TODO: Implement backward pass\n",
        "        # Start with output layer gradient\n",
        "        # Then propagate backwards through hidden layers\n",
        "        \n",
        "        pass  # Replace with your implementation\n",
        "        \n",
        "        return grads\n",
        "    \n",
        "    def update_parameters(self, grads):\n",
        "        \"\"\"\n",
        "        TODO: Update weights and biases using gradients\n",
        "        \n",
        "        For each layer:\n",
        "        W[l] = W[l] - learning_rate * dW[l]\n",
        "        b[l] = b[l] - learning_rate * db[l]\n",
        "        \"\"\"\n",
        "        # TODO: Implement parameter updates\n",
        "        pass\n",
        "    \n",
        "    def compute_loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        TODO: Compute loss\n",
        "        \n",
        "        For regression: MSE\n",
        "        For classification: Cross-entropy\n",
        "        \"\"\"\n",
        "        pass  # Replace with your implementation\n",
        "    \n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        TODO: Implement training loop\n",
        "        \n",
        "        For each iteration:\n",
        "        1. Forward propagation\n",
        "        2. Compute loss\n",
        "        3. Backward propagation\n",
        "        4. Update parameters\n",
        "        5. Store loss\n",
        "        \n",
        "        Must populate self.loss_history!\n",
        "        \"\"\"\n",
        "        self.initialize_parameters()\n",
        "        \n",
        "        for i in range(self.n_iterations):\n",
        "            # TODO: Training loop\n",
        "            pass\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        TODO: Implement prediction\n",
        "        \n",
        "        Use forward_propagation and apply appropriate thresholding\n",
        "        \"\"\"\n",
        "        pass  # Replace with your implementation\n",
        "\n",
        "print(\"‚úì MLP class defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train MLP\n",
        "print(\"Training MLP...\")\n",
        "mlp_start_time = time.time()\n",
        "\n",
        "# TODO: Define your architecture and train MLP\n",
        "mlp_architecture = []  # Example: [n_features, 16, 8, 1]\n",
        "mlp_model = MLP(architecture=mlp_architecture, learning_rate=0.01, n_iterations=1000)\n",
        "# mlp_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# TODO: Make predictions\n",
        "# mlp_predictions = mlp_model.predict(X_test_scaled)\n",
        "\n",
        "mlp_training_time = time.time() - mlp_start_time\n",
        "print(f\"‚úì MLP training completed in {mlp_training_time:.2f}s\")\n",
        "print(f\"‚úì Loss decreased from {mlp_model.loss_history[0]:.4f} to {mlp_model.loss_history[-1]:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Evaluation and Metrics\n",
        "\n",
        "Calculate appropriate metrics for your problem type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred, problem_type):\n",
        "    \"\"\"\n",
        "    TODO: Calculate appropriate metrics based on problem type\n",
        "    \n",
        "    For regression: MSE, RMSE, MAE, R¬≤\n",
        "    For classification: Accuracy, Precision, Recall, F1\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    \n",
        "    if problem_type == \"regression\":\n",
        "        # TODO: Calculate regression metrics\n",
        "        pass\n",
        "    elif problem_type in [\"binary_classification\", \"multiclass_classification\"]:\n",
        "        # TODO: Calculate classification metrics\n",
        "        pass\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Calculate metrics for both models\n",
        "# baseline_metrics = calculate_metrics(y_test, baseline_predictions, problem_type)\n",
        "# mlp_metrics = calculate_metrics(y_test, mlp_predictions, problem_type)\n",
        "\n",
        "print(\"Baseline Model Performance:\")\n",
        "# print(baseline_metrics)\n",
        "\n",
        "print(\"\\nMLP Model Performance:\")\n",
        "# print(mlp_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Visualization\n",
        "\n",
        "Create visualizations:\n",
        "1. Training loss curves\n",
        "2. Performance comparison\n",
        "3. Additional domain-specific plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Training loss curves\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# TODO: Plot baseline loss\n",
        "# plt.plot(baseline_model.loss_history, label='Baseline', color='blue')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Baseline Model - Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "# TODO: Plot MLP loss\n",
        "# plt.plot(mlp_model.loss_history, label='MLP', color='red')\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('MLP Model - Training Loss')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Performance comparison bar chart\n",
        "# TODO: Create bar chart comparing key metrics between models\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Example:\n",
        "# metrics = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
        "# baseline_scores = [baseline_metrics[m] for m in metrics]\n",
        "# mlp_scores = [mlp_metrics[m] for m in metrics]\n",
        "# \n",
        "# x = np.arange(len(metrics))\n",
        "# width = 0.35\n",
        "# \n",
        "# plt.bar(x - width/2, baseline_scores, width, label='Baseline')\n",
        "# plt.bar(x + width/2, mlp_scores, width, label='MLP')\n",
        "# plt.xlabel('Metrics')\n",
        "# plt.ylabel('Score')\n",
        "# plt.title('Model Performance Comparison')\n",
        "# plt.xticks(x, metrics)\n",
        "# plt.legend()\n",
        "# plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Analysis and Discussion\n",
        "\n",
        "Write your analysis (minimum 200 words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "analysis_text = \"\"\"\n",
        "TODO: Write your analysis here (minimum 200 words)\n",
        "\n",
        "Address these questions:\n",
        "1. Which model performed better and by how much?\n",
        "2. Why do you think one model outperformed the other?\n",
        "3. What was the computational cost difference (training time)?\n",
        "4. Any surprising findings or challenges you faced?\n",
        "5. What insights did you gain about neural networks vs linear models?\n",
        "\n",
        "Write your thoughtful analysis here. Be specific and reference your actual results.\n",
        "Compare the metrics, discuss the trade-offs, and explain what you learned.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Analysis word count: {len(analysis_text.split())} words\")\n",
        "if len(analysis_text.split()) < 200:\n",
        "    print(\"‚ö†Ô∏è  Warning: Analysis should be at least 200 words\")\n",
        "else:\n",
        "    print(\"‚úì Analysis meets word count requirement\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "---\n",
        "\n",
        "## ‚≠ê REQUIRED: Structured Output Function\n",
        "\n",
        "### **DO NOT MODIFY THE STRUCTURE BELOW**\n",
        "\n",
        "This function will be called by the auto-grader. Fill in all values accurately based on your actual results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_assignment_results():\n",
        "    \"\"\"\n",
        "    Return all assignment results in structured format.\n",
        "    \n",
        "    CRITICAL: Fill in ALL values based on your actual results!\n",
        "    This will be automatically extracted and validated.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate loss convergence flags\n",
        "    baseline_initial_loss = 0.0  # TODO: baseline_model.loss_history[0]\n",
        "    baseline_final_loss = 0.0    # TODO: baseline_model.loss_history[-1]\n",
        "    mlp_initial_loss = 0.0       # TODO: mlp_model.loss_history[0]\n",
        "    mlp_final_loss = 0.0         # TODO: mlp_model.loss_history[-1]\n",
        "    \n",
        "    results = {\n",
        "        # ===== Dataset Information =====\n",
        "        'dataset_name': dataset_name,\n",
        "        'dataset_source': dataset_source,\n",
        "        'n_samples': n_samples,\n",
        "        'n_features': n_features,\n",
        "        'problem_type': problem_type,\n",
        "        'problem_statement': problem_statement,\n",
        "        \n",
        "        # ===== Evaluation Setup =====\n",
        "        'primary_metric': primary_metric,\n",
        "        'metric_justification': metric_justification,\n",
        "        'train_samples': train_samples,\n",
        "        'test_samples': test_samples,\n",
        "        'train_test_ratio': train_test_ratio,\n",
        "        \n",
        "        # ===== Baseline Model Results =====\n",
        "        'baseline_model': {\n",
        "            'model_type': '',  # 'linear_regression', 'logistic_regression', or 'softmax_regression'\n",
        "            'learning_rate': 0.0,\n",
        "            'n_iterations': 0,\n",
        "            'initial_loss': baseline_initial_loss,\n",
        "            'final_loss': baseline_final_loss,\n",
        "            'training_time_seconds': baseline_training_time,\n",
        "            \n",
        "            # Metrics (fill based on your problem type)\n",
        "            'test_accuracy': 0.0,      # For classification\n",
        "            'test_precision': 0.0,     # For classification\n",
        "            'test_recall': 0.0,        # For classification\n",
        "            'test_f1': 0.0,            # For classification\n",
        "            'test_mse': 0.0,           # For regression\n",
        "            'test_rmse': 0.0,          # For regression\n",
        "            'test_mae': 0.0,           # For regression\n",
        "            'test_r2': 0.0,            # For regression\n",
        "        },\n",
        "        \n",
        "        # ===== MLP Model Results =====\n",
        "        'mlp_model': {\n",
        "            'architecture': mlp_architecture,\n",
        "            'n_hidden_layers': len(mlp_architecture) - 2 if len(mlp_architecture) > 0 else 0,\n",
        "            'total_parameters': 0,     # TODO: Calculate total weights + biases\n",
        "            'learning_rate': 0.0,\n",
        "            'n_iterations': 0,\n",
        "            'initial_loss': mlp_initial_loss,\n",
        "            'final_loss': mlp_final_loss,\n",
        "            'training_time_seconds': mlp_training_time,\n",
        "            \n",
        "            # Metrics\n",
        "            'test_accuracy': 0.0,\n",
        "            'test_precision': 0.0,\n",
        "            'test_recall': 0.0,\n",
        "            'test_f1': 0.0,\n",
        "            'test_mse': 0.0,\n",
        "            'test_rmse': 0.0,\n",
        "            'test_mae': 0.0,\n",
        "            'test_r2': 0.0,\n",
        "        },\n",
        "        \n",
        "        # ===== Comparison =====\n",
        "        'improvement': 0.0,            # MLP primary_metric - baseline primary_metric\n",
        "        'improvement_percentage': 0.0,  # (improvement / baseline) * 100\n",
        "        'baseline_better': False,       # True if baseline outperformed MLP\n",
        "        \n",
        "        # ===== Analysis =====\n",
        "        'analysis': analysis_text,\n",
        "        'analysis_word_count': len(analysis_text.split()),\n",
        "        \n",
        "        # ===== Loss Convergence Flags =====\n",
        "        'baseline_loss_decreased': baseline_final_loss < baseline_initial_loss,\n",
        "        'mlp_loss_decreased': mlp_final_loss < mlp_initial_loss,\n",
        "        'baseline_converged': False,  # Optional: True if converged\n",
        "        'mlp_converged': False,\n",
        "    }\n",
        "    \n",
        "    return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Your Output\n",
        "\n",
        "Run this cell to verify your results dictionary is complete and properly formatted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the output\n",
        "import json\n",
        "\n",
        "try:\n",
        "    results = get_assignment_results()\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"ASSIGNMENT RESULTS SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "    print(json.dumps(results, indent=2, default=str))\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    \n",
        "    # Check for missing values\n",
        "    missing = []\n",
        "    def check_dict(d, prefix=\"\"):\n",
        "        for k, v in d.items():\n",
        "            if isinstance(v, dict):\n",
        "                check_dict(v, f\"{prefix}{k}.\")\n",
        "            elif (v == 0 or v == \"\" or v == 0.0 or v == []) and \\\n",
        "                 k not in ['improvement', 'improvement_percentage', 'baseline_better', \n",
        "                          'baseline_converged', 'mlp_converged', 'total_parameters',\n",
        "                          'test_accuracy', 'test_precision', 'test_recall', 'test_f1',\n",
        "                          'test_mse', 'test_rmse', 'test_mae', 'test_r2']:\n",
        "                missing.append(f\"{prefix}{k}\")\n",
        "    \n",
        "    check_dict(results)\n",
        "    \n",
        "    if missing:\n",
        "        print(f\"‚ö†Ô∏è  Warning: {len(missing)} fields still need to be filled:\")\n",
        "        for m in missing[:15]:  # Show first 15\n",
        "            print(f\"  - {m}\")\n",
        "        if len(missing) > 15:\n",
        "            print(f\"  ... and {len(missing)-15} more\")\n",
        "    else:\n",
        "        print(\"‚úÖ All required fields are filled!\")\n",
        "        print(\"\\nüéâ You're ready to submit!\")\n",
        "        print(\"\\nNext steps:\")\n",
        "        print(\"1. Kernel ‚Üí Restart & Clear Output\")\n",
        "        print(\"2. Kernel ‚Üí Restart & Run All\")\n",
        "        print(\"3. Verify no errors\")\n",
        "        print(\"4. Save notebook\")\n",
        "        print(\"5. Rename as: YourStudentID_assignment.ipynb\")\n",
        "        print(\"6. Submit to LMS\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in get_assignment_results(): {str(e)}\")\n",
        "    print(\"\\nPlease fix the errors above before submitting.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## üì§ Before Submitting - Final Checklist\n",
        "\n",
        "- [ ] **All TODO sections completed**\n",
        "- [ ] **Both models implemented from scratch** (no sklearn models!)\n",
        "- [ ] **get_assignment_results() function filled accurately**\n",
        "- [ ] **Loss decreases for both models**\n",
        "- [ ] **Analysis ‚â• 200 words**\n",
        "- [ ] **All cells run without errors** (Restart & Run All)\n",
        "- [ ] **Visualizations created**\n",
        "- [ ] **File renamed correctly**: YourStudentID_assignment.ipynb\n",
        "\n",
        "---\n",
        "\n",
        "## ‚è≠Ô∏è What Happens Next\n",
        "\n",
        "After submission:\n",
        "1. ‚úÖ Your notebook will be **auto-graded** (executes automatically)\n",
        "2. ‚úÖ You'll receive a **verification quiz** (10 questions, 5 minutes)\n",
        "3. ‚úÖ Quiz questions based on **YOUR specific results**\n",
        "4. ‚úÖ Final score released after quiz validation\n",
        "\n",
        "**The verification quiz ensures you actually ran your code!**\n",
        "\n",
        "---\n",
        "\n",
        "**Good luck! üöÄ**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
